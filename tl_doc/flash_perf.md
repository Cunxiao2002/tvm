The flash-attention performance on RTX-4090 GPU, with cuda toolkit 12.2

Use fp16->fp32 mma, value in TFlops, higher is better.

| N_CTX,DIM | Flash_attn | Tvm.tl |
| --------- | ---------- | ------ |
| 512, 64   | 155.64     | 155.09 |
| 1024, 64  | 166.98     | 163.69 |
| 2048, 64  | 168.06     | 163.45 |
| 512, 128  | 153.14     | 155.6  |
| 1024, 128 | 166.15     | 163.64 |
| 2048, 128 | 169.28     | 166.79 |
| 512, 256  | 143.92     | 146.25 |
| 1024, 256 | 152.3      | 157.62 |
| 2048, 256 | 156.15     | 163.48 |
